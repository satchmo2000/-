# 分别输出测试集x_test，标签y_test，预测结果y_pred
# 输出到csv文件，文件名=file
def output(x_test , y_test , y_pred , file):
    x_test_full = pd.DataFrame(np.array(x_test))

    if not y_test is None:
        y_test_array = np.array(y_test)
        y_test_d = pd.DataFrame(y_test_array)
        x_test_full["label"] = y_test_d

    y_pred_d = pd.DataFrame(y_pred)
    x_test_full["check"] = y_pred_d.astype(np.int16)

    x_test_full.to_csv(file)

def label_split(label):
    #分类信息转置
    label2 = []
    for l in label:

        if l == 0:
            label2.append([1,0,0,0,0,0,0,0,0,0])
        elif l == 1:
            label2.append([0,1,0,0,0,0,0,0,0,0])
        elif l == 2:
            label2.append([0,0,1,0,0,0,0,0,0,0])
        elif l == 3:
            label2.append([0,0,0,1,0,0,0,0,0,0])
        elif l == 4:
            label2.append([0,0,0,0,1,0,0,0,0,0])
        elif l == 5:
            label2.append([0,0,0,0,0,1,0,0,0,0])
        elif l == 6:
            label2.append([0,0,0,0,0,0,1,0,0,0])
        elif l == 7:
            label2.append([0,0,0,0,0,0,0,1,0,0])
        elif l == 8:
            label2.append([0,0,0,0,0,0,0,0,1,0])
        elif l == 9:
            label2.append([0,0,0,0,0,0,0,0,0,1])
    label2 = np.array(label2)
    return label2

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()

def loaddata(file , split = True):
    data = pd.read_csv(file , header=0)
    x = data.iloc[:,1:-1]
    x = abs(np.fft.fft(x)[:,:3001])/6000
    x = ss.fit_transform(x)
    y = np.array(data['label'])
    if split:
        y = label_split(y)
    return x , y

from __future__ import absolute_import, division, print_function

import tensorflow as tf
from tensorflow.keras import Model, layers

#配置神经网络参数
n_hidden_1 = 64
n_hidden_2 = 256
n_classes = 10
n_input = 3001
eta = 0.001

max_epoch = 4000
display_step = 100

class NeuralNet(Model):
    def __init__(self):
        super(NeuralNet, self).__init__()
        self.fc1 = layers.Dense(n_hidden_1 , activation = tf.nn.relu)
        self.fc2 = layers.Dense(n_hidden_2 , activation = tf.nn.relu)
        self.out = layers.Dense(n_classes , activation=tf.nn.softmax)
    
    def call(self, x , is_training = False):
        x = self.fc1(x)
        #x = self.fc2(x)
        x = self.out(x)
        if not is_training:
            x = tf.argmax(x , 1)
        
        return x

neural_net = NeuralNet()

#y_true为10列的值，需要通过argmax(y_true,1)才能变成结果
def cross_entropy_loss(y_hat, y_true):
    y_true = tf.cast(y_true, tf.int64)
    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_hat)
    return tf.reduce_mean(loss)

def accuracy(y_pred , y_true):
    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))
    return tf.reduce_mean(tf.cast(correct_prediction , tf.float32), axis = -1)

optimizer = tf.keras.optimizers.SGD(eta)

#优化
def run_optimization(x, y):
    with tf.GradientTape() as g:
        pred = neural_net(x , is_training = True)
        loss = cross_entropy_loss(pred,y)
        acc = accuracy(pred, y)

    trainable_variables = neural_net.trainable_variables
    gradients = g.gradient(loss, trainable_variables)
    
    optimizer.apply_gradients(zip(gradients, trainable_variables))
    
#训练数据读取并拆分
data_x , data_y = loaddata('./zcdata/train.csv' , split = True)
x_train,x_test,y_train,y_test = train_test_split(data_x,data_y,random_state=1,test_size=0.2)

验证数据读取
test_x , test_y = loaddata('./zcdata/test.csv' , split = True)

#训练
x_train_float32 = tf.cast(x_train , tf.float32)
x_test_float32 = tf.cast(x_test , tf.float32)
test_x_float32 = tf.cast(test_x , tf.float32)

for epoch in range(max_epoch):
    
    if epoch == -1:
        pred = neural_net(x_train_float32 , is_training = True)
        print(pred)
        print(y_train)
        loss = cross_entropy_loss(pred,y_train)
    
    run_optimization(x_train_float32 , y_train)
    
    if epoch % display_step == 0:
        pred = neural_net(x_train_float32 , is_training = True)
        loss = cross_entropy_loss(pred,y_train)
        acc = accuracy(pred, y_train)

        pred2 = neural_net(x_test_float32 , is_training = True)
        loss2 = cross_entropy_loss(pred2,y_test)
        acc2 = accuracy(pred2, y_test)

        pred3 = neural_net(test_x_float32 , is_training = True)
        loss3 = cross_entropy_loss(pred3,test_y)
        acc3 = accuracy(pred3, test_y)
        
        print("step: %i,loss:%f,accuracy:%f,loss2:%f,accuracy2:%f,loss3:%f,accuracy3:%f" %(epoch , loss , acc , loss2 , acc2 , loss3 , acc3))
        
    pred3num = tf.argmax(pred3 , 1).numpy()
test_y_num = tf.argmax(test_y , 1).numpy()
pred3sub = pred3num - test_y_num
pred3sub
